# Proyecto de SegmentaciÃ³n Laboral con K-Means

## ğŸ“‹ DescripciÃ³n General

Este proyecto implementa un sistema completo de segmentaciÃ³n de trabajadores mediante tÃ©cnicas de clustering (K-Means) con una API REST para realizar predicciones en tiempo real. Permite identificar distintos perfiles laborales a partir de caracterÃ­sticas sociodemogrÃ¡ficas y condiciones de empleo.

La soluciÃ³n incluye:
- Pipeline completo de preprocesamiento, reducciÃ³n dimensional y entrenamiento
- Herramientas de evaluaciÃ³n y visualizaciÃ³n de clusters
- API REST con FastAPI para realizar predicciones
- Sistema de autenticaciÃ³n mediante API Keys

## ğŸ—ï¸ Estructura del Proyecto

```
employ-kmeans/
â”‚
â”œâ”€â”€ app.py                     # API REST con FastAPI
â”œâ”€â”€ .env                       # Variables de entorno (API KEY)
â”œâ”€â”€ run_pipeline.py            # Script para ejecutar el pipeline completo
â”‚
â”œâ”€â”€ config/                    # ConfiguraciÃ³n de rutas
â”‚   â””â”€â”€ paths.py               # DefiniciÃ³n de rutas a archivos
â”‚
â”œâ”€â”€ pipeline/                  # MÃ³dulos del pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py         # Carga y filtrado inicial
â”‚   â”œâ”€â”€ preprocessor.py        # TransformaciÃ³n de variables
â”‚   â”œâ”€â”€ dimensionality_reducer.py  # ReducciÃ³n PCA
â”‚   â”œâ”€â”€ k_finder.py            # DeterminaciÃ³n de k Ã³ptimo
â”‚   â”œâ”€â”€ trainer.py             # Entrenamiento del modelo
â”‚   â””â”€â”€ evaluator.py           # EvaluaciÃ³n y reportes
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Datos originales
â”‚   â”‚   â””â”€â”€ employ25_3-4-5.csv
â”‚   â””â”€â”€ processed/             # Datos procesados
â”‚       â”œâ”€â”€ employ_processed.csv
â”‚       â”œâ”€â”€ employ_preprocessed_KMEANS.csv
â”‚       â”œâ”€â”€ employ_pca_9d.csv
â”‚       â””â”€â”€ employ_clustered_k4.csv
â”‚
â”œâ”€â”€ models/                    # Modelos entrenados
â”‚   â”œâ”€â”€ kmeans_model.joblib
â”‚   â””â”€â”€ kmeans_model.pkl
â”‚
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â””â”€â”€ tryend.ipynb
â”‚
â”œâ”€â”€ logs/                      # Registros de ejecuciÃ³n
â”‚   â””â”€â”€ execution.log
â”‚
â””â”€â”€ reports/                   # Reportes y visualizaciones
    â”œâ”€â”€ cluster_analysis.csv
    â”œâ”€â”€ metrics.txt
    â””â”€â”€ visuals/
        â”œâ”€â”€ elbow_plot.png
        â”œâ”€â”€ pca_variance.png
        â””â”€â”€ silhouette_plot.png
```

## ğŸš€ Primeros Pasos

### Requisitos Previos

- Python 3.8+
- Dependencias principales:
  - pandas
  - scikit-learn
  - fastapi
  - joblib
  - python-dotenv
  - matplotlib
  - seaborn

### InstalaciÃ³n

1. Clonar el repositorio:
```bash
git clone https://github.com/Angel226m/employ-kmeans.git
cd employ-kmeans
```

2. Instalar dependencias:
```bash
pip install -r requirements.txt
```

3. Configurar la API Key para el servicio:
```bash
# Crear archivo .env con la siguiente variable
API_KEY=tu_clave_secreta_aquÃ­
```

## ğŸ’» Uso del Sistema

### Entrenamiento del Modelo

Para ejecutar el pipeline completo de entrenamiento:

```bash
python run_pipeline.py
```

Este script ejecuta secuencialmente:
1. Carga y filtrado de datos iniciales
2. Preprocesamiento (transformaciÃ³n de variables)
3. ReducciÃ³n dimensional con PCA
4. DeterminaciÃ³n del k Ã³ptimo para K-Means
5. Entrenamiento del modelo
6. EvaluaciÃ³n y generaciÃ³n de reportes

Durante la ejecuciÃ³n, el sistema solicitarÃ¡ un valor para k (nÃºmero de clusters):
```
[SUGERENCIA] K Ã³ptimo calculado: 4
Ingrese el valor de k a utilizar: 
```

### EjecuciÃ³n de la API REST

Para iniciar el servidor de la API:

```bash
uvicorn app:app --reload
```

La API estarÃ¡ disponible en: http://localhost:8000

Endpoint principal:
- **POST /predecir**: Realiza una predicciÃ³n del cluster al que pertenece un trabajador

Ejemplo de solicitud:
```json
{
  "SEXO": 1,
  "TUVO_TRABAJO": 1,
  "BUSCA_OTRO_TRABAJO": 0,
  "QUIERE_MAS_HORAS": 0,
  "SEGURO_SALUD": 1,
  "LIMIT_MOVIMIENTO": 0,
  "LIMIT_VISION": 0,
  "LIMIT_COMUNICACION": 0,
  "LIMIT_AUDICION": 0,
  "LIMIT_APRENDIZAJE": 0,
  "LIMIT_RELACION": 0,
  "EDAD": 45,
  "INGRESO_PRINCIPAL": 3500.0,
  "DIAS_AUSENTE": 0,
  "NIVEL_EDUCATIVO": 11,
  "TAMANO_EMPRESA": 3,
  "ETNIA": "1",
  "TIPO_TRABAJADOR": "1.0",
  "REGISTRO_SUNAT": "1.0",
  "LIBROS_CONTABLES": "1.0",
  "FRECUENCIA_PAGO": "1.0",
  "NIVEL_OCUPACION": "1"
}
```

Respuesta:
```json
{
  "cluster": 2,
  "perfil": "Profesionales y Empleados Formales de Altos Ingresos y Estables"
}
```

**IMPORTANTE**: Todas las solicitudes deben incluir el header `X-API-Key` con la clave configurada.

### PredicciÃ³n por Lotes

Para procesar un archivo CSV con mÃºltiples registros:

1. Preparar un archivo CSV con las mismas columnas que el dataset original
2. Ejecutar el script de predicciÃ³n por lotes:
```bash
python notebooks/pred_batch.py --input input_file.csv --output predictions.csv
```

## ğŸ” Detalles TÃ©cnicos

### Preprocesamiento de Datos

El pipeline aplica las siguientes transformaciones:

1. **RecodificaciÃ³n de variables binarias**:
   - ConversiÃ³n de valores `2` a `0` en variables como SEXO, TUVO_TRABAJO, etc.

2. **CreaciÃ³n de variables derivadas**:
   - Variable `DISCAPACIDAD` basada en seis limitaciones fÃ­sicas/cognitivas

3. **One-hot encoding**:
   - TransformaciÃ³n de variables categÃ³ricas como ETNIA, TIPO_TRABAJADOR, etc.

4. **Escalado de variables numÃ©ricas**:
   - EstandarizaciÃ³n de EDAD, INGRESO_PRINCIPAL y DIAS_AUSENTE

5. **ImputaciÃ³n de valores faltantes**:
   - MÃ©todo MICE (Multiple Imputation by Chained Equations) para NaN

### ReducciÃ³n Dimensional

Para optimizar el clustering se aplica PCA (AnÃ¡lisis de Componentes Principales):

- ReducciÃ³n a 9 dimensiones principales
- PreservaciÃ³n aproximada del 85% de la varianza original
- Mejora de la separabilidad entre clusters

### Modelo de Clustering

Se utiliza K-Means con las siguientes caracterÃ­sticas:

- NÃºmero de clusters determinado mediante mÃ©todo del codo y coeficiente de silueta
- InicializaciÃ³n optimizada (k-means++)
- MÃ©tricas de evaluaciÃ³n:
  - Silhouette Score
  - Calinski-Harabasz Index
  - Davies-Bouldin Score

### Perfiles de Clusters Identificados

El sistema identifica cuatro perfiles principales:

1. **Adultos Mayores con Baja EducaciÃ³n y Alta Formalidad en Salud**
   - CaracterÃ­sticas: Mayor edad, educaciÃ³n bÃ¡sica, alta cobertura de salud

2. **Trabajadores Informales de Baja CalificaciÃ³n y Menores Ingresos**
   - CaracterÃ­sticas: Sin registro formal, ingresos bajos, empleos temporales

3. **Profesionales y Empleados Formales de Altos Ingresos y Estables**
   - CaracterÃ­sticas: Alta educaciÃ³n, ingresos elevados, empleo estable

4. **JÃ³venes Calificados en Empleos con Riesgo de Subempleo/Informalidad**
   - CaracterÃ­sticas: EducaciÃ³n media-alta, ingresos medios, empleos variados

### API REST

Desarrollada con FastAPI, la API ofrece:

- SerializaciÃ³n/deserializaciÃ³n automÃ¡tica con Pydantic
- AutenticaciÃ³n mediante API Keys (header `X-API-Key`)
- DocumentaciÃ³n automÃ¡tica (disponible en `/docs`)
- ValidaciÃ³n de datos de entrada
- Manejo de excepciones y errores HTTP

## ğŸ“Š EvaluaciÃ³n y Reportes

El sistema genera automÃ¡ticamente:

1. **MÃ©tricas de calidad del clustering**:
   - Archivo `reports/metrics.txt` con indicadores clave

2. **AnÃ¡lisis estadÃ­stico por cluster**:
   - Archivo `reports/cluster_analysis.csv` con estadÃ­sticas descriptivas

3. **Visualizaciones**:
   - MÃ©todo del codo para determinar k Ã³ptimo
   - Coeficiente de silueta por valor de k
   - Varianza explicada por PCA
   - DistribuciÃ³n de variables por cluster

## ğŸ‘¨â€ğŸ’» ContribuciÃ³n y Desarrollo

Para contribuir al proyecto:

1. Crear un fork del repositorio
2. Crear una rama para tu funcionalidad (`git checkout -b feature/nueva-funcionalidad`)
3. Realizar cambios y commits (`git commit -am 'AÃ±adir nueva funcionalidad'`)
4. Enviar cambios a tu fork (`git push origin feature/nueva-funcionalidad`)
5. Crear un Pull Request

## ğŸ“œ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo LICENSE para mÃ¡s detalles.

## ğŸ”— Referencias

- Scikit-learn: https://scikit-learn.org/
- FastAPI: https://fastapi.tiangolo.com/
- Pandas: https://pandas.pydata.org/
- Joblib: https://joblib.readthedocs.io/
