{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac85b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Cargando modelo...\n",
      "üìä Cargando datos...\n",
      "üîß Aplicando preprocesamiento...\n",
      "‚úÖ Datos procesados: (25, 36)\n",
      "üìã Columnas procesadas: ['SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO', 'TAMANO_EMPRESA', 'QUIERE_MAS_HORAS', 'INGRESO_PRINCIPAL', 'SEGURO_SALUD', 'DIAS_AUSENTE', 'DISCAPACIDAD', 'ETNIA_2', 'ETNIA_3', 'ETNIA_4', 'ETNIA_5', 'ETNIA_6', 'ETNIA_7', 'ETNIA_8', 'ETNIA_9', 'TIPO_TRABAJADOR_2.0', 'TIPO_TRABAJADOR_3.0', 'TIPO_TRABAJADOR_4.0', 'TIPO_TRABAJADOR_6.0', 'TIPO_TRABAJADOR_7.0', 'TIPO_TRABAJADOR_8.0', 'TIPO_TRABAJADOR_9.0', 'NIVEL_OCUPACION_2', 'NIVEL_OCUPACION_3', 'NIVEL_OCUPACION_4', 'REGISTRO_SUNAT_2.0', 'REGISTRO_SUNAT_3.0', 'LIBROS_CONTABLES_2.0', 'FRECUENCIA_PAGO_2.0', 'FRECUENCIA_PAGO_3.0', 'FRECUENCIA_PAGO_4.0', 'FRECUENCIA_PAGO_5.0']\n",
      "üéØ Realizando predicci√≥n...\n",
      "‚úÖ ¬°Predicci√≥n completada exitosamente!\n",
      "üìà Distribuci√≥n de clusters:\n",
      "   Cluster 0: 10 registros\n",
      "   Cluster 1: 14 registros\n",
      "   Cluster 2: 1 registros\n",
      "üíæ Resultados guardados en: resultados_con_clusters.csv\n",
      "\n",
      "üìù Muestra de resultados:\n",
      "   SEXO  EDAD  NIVEL_EDUCATIVO  Cluster\n",
      "0     2    24               10        1\n",
      "1     1    78               11        1\n",
      "2     1    21                8        0\n",
      "3     2    22               10        1\n",
      "4     1    43                6        0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# 1. Cargar el modelo\n",
    "print(\"üì¶ Cargando modelo...\")\n",
    "pipeline = joblib.load('../models/kmeans_model__oldc.joblib')\n",
    "\n",
    "# 2. Cargar nuevos datos\n",
    "print(\"üìä Cargando datos...\")\n",
    "new_data = pd.read_csv('new_data_sample.csv')\n",
    "\n",
    "# 3. Funci√≥n de preprocesamiento (debe coincidir exactamente con el entrenamiento)\n",
    "def preprocess_for_prediction(df):\n",
    "    \"\"\"\n",
    "    Aplica el mismo preprocesamiento que se us√≥ durante el entrenamiento\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Recodificar variables binarias (2 -> 0)\n",
    "    binary_cols = ['SEXO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO', 'QUIERE_MAS_HORAS', 'SEGURO_SALUD']\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({2: 0})\n",
    "    \n",
    "    # Crear variable DISCAPACIDAD a partir de las limitaciones\n",
    "    disability_cols = ['LIMIT_MOVIMIENTO', 'LIMIT_VISION', 'LIMIT_COMUNICACION', \n",
    "                      'LIMIT_AUDICION', 'LIMIT_APRENDIZAJE', 'LIMIT_RELACION']\n",
    "    \n",
    "    if all(col in df.columns for col in disability_cols):\n",
    "        df['DISCAPACIDAD'] = df[disability_cols].sum(axis=1).apply(lambda x: 1 if x >= 1 else 0)\n",
    "        df = df.drop(columns=disability_cols)\n",
    "    \n",
    "    # One-hot encoding para variables categ√≥ricas\n",
    "    categorical_cols = ['ETNIA', 'TIPO_TRABAJADOR', 'NIVEL_OCUPACION', \n",
    "                       'REGISTRO_SUNAT', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True, dummy_na=True)\n",
    "    \n",
    "    # Columnas que el modelo espera (basado en el modelo cargado)\n",
    "    expected_features = [\n",
    "        'SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO',\n",
    "        'TAMANO_EMPRESA', 'QUIERE_MAS_HORAS', 'INGRESO_PRINCIPAL', 'SEGURO_SALUD',\n",
    "        'DIAS_AUSENTE', 'DISCAPACIDAD', 'ETNIA_2', 'ETNIA_3', 'ETNIA_4', 'ETNIA_5',\n",
    "        'ETNIA_6', 'ETNIA_7', 'ETNIA_8', 'ETNIA_9', 'TIPO_TRABAJADOR_2.0',\n",
    "        'TIPO_TRABAJADOR_3.0', 'TIPO_TRABAJADOR_4.0', 'TIPO_TRABAJADOR_6.0',\n",
    "        'TIPO_TRABAJADOR_7.0', 'TIPO_TRABAJADOR_8.0', 'TIPO_TRABAJADOR_9.0',\n",
    "        'NIVEL_OCUPACION_2', 'NIVEL_OCUPACION_3', 'NIVEL_OCUPACION_4',\n",
    "        'REGISTRO_SUNAT_2.0', 'REGISTRO_SUNAT_3.0', 'LIBROS_CONTABLES_2.0',\n",
    "        'FRECUENCIA_PAGO_2.0', 'FRECUENCIA_PAGO_3.0', 'FRECUENCIA_PAGO_4.0',\n",
    "        'FRECUENCIA_PAGO_5.0'\n",
    "    ]\n",
    "    \n",
    "    # Asegurar que todas las columnas esperadas est√©n presentes\n",
    "    for col in expected_features:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    # Retornar solo las columnas en el orden correcto\n",
    "    return df[expected_features]\n",
    "\n",
    "# 4. Aplicar preprocesamiento\n",
    "print(\"üîß Aplicando preprocesamiento...\")\n",
    "try:\n",
    "    processed_data = preprocess_for_prediction(new_data)\n",
    "    print(f\"‚úÖ Datos procesados: {processed_data.shape}\")\n",
    "    print(f\"üìã Columnas procesadas: {list(processed_data.columns)}\")\n",
    "    \n",
    "    # 5. Hacer predicci√≥n\n",
    "    print(\"üéØ Realizando predicci√≥n...\")\n",
    "    clusters = pipeline.predict(processed_data)\n",
    "    \n",
    "    # 6. Agregar resultados a los datos originales\n",
    "    new_data['Cluster'] = clusters\n",
    "    \n",
    "    # 7. Guardar resultados\n",
    "    output_file = 'resultados_con_clusters.csv'\n",
    "    new_data.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"‚úÖ ¬°Predicci√≥n completada exitosamente!\")\n",
    "    print(f\"üìà Distribuci√≥n de clusters:\")\n",
    "    cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        print(f\"   Cluster {cluster}: {count} registros\")\n",
    "    \n",
    "    print(f\"üíæ Resultados guardados en: {output_file}\")\n",
    "    \n",
    "    # Mostrar una muestra de los resultados\n",
    "    print(\"\\nüìù Muestra de resultados:\")\n",
    "    print(new_data[['SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'Cluster']].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"üö® ERROR: {e}\")\n",
    "    print(\"\\nüîç Informaci√≥n de depuraci√≥n:\")\n",
    "    print(f\"Forma de los datos originales: {new_data.shape}\")\n",
    "    print(f\"Columnas disponibles: {list(new_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "568fb0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Cargando modelo...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 1. Cargar el modelo\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müì¶ Cargando modelo...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../models/kmeans_model.joblib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2. Cargar nuevos datos\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìä Cargando datos...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\numpy_pickle.py:749\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[0;32m    744\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m    746\u001b[0m             \u001b[38;5;66;03m# A memory-mapped array has to be mapped with the endianness\u001b[39;00m\n\u001b[0;32m    747\u001b[0m             \u001b[38;5;66;03m# it has been written with. Other arrays are coerced to the\u001b[39;00m\n\u001b[0;32m    748\u001b[0m             \u001b[38;5;66;03m# native endianness of the host system.\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m                \u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidated_mmap_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\numpy_pickle.py:626\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    624\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 626\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    628\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    630\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    633\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    634\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pickle.py:1255\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1255\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pickle.py:1580\u001b[0m, in \u001b[0;36m_Unpickler.load_stack_global\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m   1579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTACK_GLOBAL requires str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pickle.py:1621\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m   1619\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING:\n\u001b[0;32m   1620\u001b[0m         module \u001b[38;5;241m=\u001b[39m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING[module]\n\u001b[1;32m-> 1621\u001b[0m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m   1623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _getattribute(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pipeline'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pipeline import \n",
    "\n",
    "\n",
    "# 1. Cargar el modelo\n",
    "print(\"üì¶ Cargando modelo...\")\n",
    "pipeline = joblib.load('../models/kmeans_model.joblib')\n",
    "\n",
    "# 2. Cargar nuevos datos\n",
    "print(\"üìä Cargando datos...\")\n",
    "new_data = pd.read_csv('new_data_sample.csv')\n",
    "\n",
    "# 3. Funci√≥n de preprocesamiento (debe coincidir exactamente con el entrenamiento)\n",
    "def preprocess_for_prediction(df):\n",
    "    \"\"\"\n",
    "    Aplica el mismo preprocesamiento que se us√≥ durante el entrenamiento\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Recodificar variables binarias (2 -> 0)\n",
    "    binary_cols = ['SEXO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO', 'QUIERE_MAS_HORAS', 'SEGURO_SALUD']\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({2: 0})\n",
    "    \n",
    "    # Crear variable DISCAPACIDAD a partir de las limitaciones\n",
    "    disability_cols = ['LIMIT_MOVIMIENTO', 'LIMIT_VISION', 'LIMIT_COMUNICACION', \n",
    "                      'LIMIT_AUDICION', 'LIMIT_APRENDIZAJE', 'LIMIT_RELACION']\n",
    "    \n",
    "    if all(col in df.columns for col in disability_cols):\n",
    "        df['DISCAPACIDAD'] = df[disability_cols].sum(axis=1).apply(lambda x: 1 if x >= 1 else 0)\n",
    "        df = df.drop(columns=disability_cols)\n",
    "    \n",
    "    # One-hot encoding para variables categ√≥ricas\n",
    "    categorical_cols = ['ETNIA', 'TIPO_TRABAJADOR', 'NIVEL_OCUPACION', \n",
    "                       'REGISTRO_SUNAT', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True, dummy_na=True)\n",
    "    \n",
    "    # Columnas que el modelo espera (basado en el modelo cargado)\n",
    "    expected_features = [\n",
    "        'SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO',\n",
    "        'TAMANO_EMPRESA', 'QUIERE_MAS_HORAS', 'INGRESO_PRINCIPAL', 'SEGURO_SALUD',\n",
    "        'DIAS_AUSENTE', 'DISCAPACIDAD', 'ETNIA_2', 'ETNIA_3', 'ETNIA_4', 'ETNIA_5',\n",
    "        'ETNIA_6', 'ETNIA_7', 'ETNIA_8', 'ETNIA_9', 'TIPO_TRABAJADOR_2.0',\n",
    "        'TIPO_TRABAJADOR_3.0', 'TIPO_TRABAJADOR_4.0', 'TIPO_TRABAJADOR_6.0',\n",
    "        'TIPO_TRABAJADOR_7.0', 'TIPO_TRABAJADOR_8.0', 'TIPO_TRABAJADOR_9.0',\n",
    "        'NIVEL_OCUPACION_2', 'NIVEL_OCUPACION_3', 'NIVEL_OCUPACION_4',\n",
    "        'REGISTRO_SUNAT_2.0', 'REGISTRO_SUNAT_3.0', 'LIBROS_CONTABLES_2.0',\n",
    "        'FRECUENCIA_PAGO_2.0', 'FRECUENCIA_PAGO_3.0', 'FRECUENCIA_PAGO_4.0',\n",
    "        'FRECUENCIA_PAGO_5.0'\n",
    "    ]\n",
    "    \n",
    "    # Asegurar que todas las columnas esperadas est√©n presentes\n",
    "    for col in expected_features:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    # Retornar solo las columnas en el orden correcto\n",
    "    return df[expected_features]\n",
    "\n",
    "# 4. Aplicar preprocesamiento\n",
    "print(\"üîß Aplicando preprocesamiento...\")\n",
    "try:\n",
    "    processed_data = preprocess_for_prediction(new_data)\n",
    "    print(f\"‚úÖ Datos procesados: {processed_data.shape}\")\n",
    "    print(f\"üìã Columnas procesadas: {list(processed_data.columns)}\")\n",
    "    \n",
    "    # 5. Hacer predicci√≥n\n",
    "    print(\"üéØ Realizando predicci√≥n...\")\n",
    "    clusters = pipeline.predict(processed_data)\n",
    "    \n",
    "    # 6. Agregar resultados a los datos originales\n",
    "    new_data['Cluster'] = clusters\n",
    "    \n",
    "    # 7. Guardar resultados\n",
    "    output_file = 'resultados_con_clusters.csv'\n",
    "    new_data.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"‚úÖ ¬°Predicci√≥n completada exitosamente!\")\n",
    "    print(f\"üìà Distribuci√≥n de clusters:\")\n",
    "    cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        print(f\"   Cluster {cluster}: {count} registros\")\n",
    "    \n",
    "    print(f\"üíæ Resultados guardados en: {output_file}\")\n",
    "    \n",
    "    # Mostrar una muestra de los resultados\n",
    "    print(\"\\nüìù Muestra de resultados:\")\n",
    "    print(new_data[['SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'Cluster']].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"üö® ERROR: {e}\")\n",
    "    print(\"\\nüîç Informaci√≥n de depuraci√≥n:\")\n",
    "    print(f\"Forma de los datos originales: {new_data.shape}\")\n",
    "    print(f\"Columnas disponibles: {list(new_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a708ea3e",
   "metadata": {},
   "source": [
    "NUEVO MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10139e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ü§ñ USANDO EL MODELO NUEVO - kmeans_model.joblib\n",
      "============================================================\n",
      "üìä Cargando datos sin procesar...\n",
      "‚úÖ Datos crudos cargados: (25, 22)\n",
      "üìã Columnas disponibles: ['C207', 'C208', 'C366', 'C377', 'C303', 'C310', 'C335', 'OCUP300', 'C312', 'C317', 'C333', 'C313', 'C338', 'ingtrabw', 'C375_1', 'C375_2', 'C375_3', 'C375_4', 'C375_5', 'C375_6', 'SEGURO1', 'C205']\n",
      "\n",
      "üìù Vista previa de los datos:\n",
      "   C207  C208  C366  C377  C303  C310  C335  OCUP300  C312  C317  ...  C338  \\\n",
      "0     2    24    10     7     2   NaN   NaN        4   NaN   NaN  ...   NaN   \n",
      "1     1    78    11     1     2   NaN   NaN        4   NaN   NaN  ...   NaN   \n",
      "2     1    21     8     7     1   3.0   2.0        1   2.0   3.0  ...   1.0   \n",
      "3     2    22    10     7     2   NaN   NaN        4   NaN   NaN  ...   NaN   \n",
      "4     1    43     6     7     1   3.0   2.0        1   1.0   2.0  ...   1.0   \n",
      "\n",
      "   ingtrabw  C375_1  C375_2  C375_3  C375_4  C375_5  C375_6  SEGURO1  C205  \n",
      "0       NaN       2       2       2       2       2       2        5   2.0  \n",
      "1       NaN       2       2       2       2       2       2        1   2.0  \n",
      "2    1800.0       2       2       2       2       2       2        5   2.0  \n",
      "3       NaN       2       2       2       2       2       2        5   2.0  \n",
      "4    4500.0       2       2       2       2       2       2        6   2.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "üîß Aplicando preprocesamiento manual...\n",
      "‚úÖ Datos procesados: (25, 36)\n",
      "\n",
      "üì¶ Cargando modelo NUEVO...\n",
      "‚úÖ Modelo NUEVO cargado: <class 'sklearn.pipeline.Pipeline'>\n",
      "üéØ Realizando predicci√≥n con modelo NUEVO...\n",
      "‚ùå Error: columns are missing: {'TIPO_TRABAJADOR', 'ETNIA', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO', 'NIVEL_OCUPACION', 'REGISTRO_SUNAT'}\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\destr\\AppData\\Local\\Temp\\ipykernel_14648\\3589152920.py\", line 127, in <module>\n",
      "    clusters = modelo_nuevo.predict(datos_procesados)\n",
      "  File \"c:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\pipeline.py\", line 787, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"c:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1090, in transform\n",
      "    raise ValueError(f\"columns are missing: {diff}\")\n",
      "ValueError: columns are missing: {'TIPO_TRABAJADOR', 'ETNIA', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO', 'NIVEL_OCUPACION', 'REGISTRO_SUNAT'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # PATH funcionando ‚úÖ\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ü§ñ USANDO EL MODELO NUEVO - kmeans_model.joblib\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Cargar datos sin procesar\n",
    "print(\"üìä Cargando datos sin procesar...\")\n",
    "try:\n",
    "    datos_crudos = pd.read_csv('notebooks/new_data_sample_SINP.csv', na_values=[\" \"])\n",
    "    \n",
    "    print(f\"‚úÖ Datos crudos cargados: {datos_crudos.shape}\")\n",
    "    print(f\"üìã Columnas disponibles: {list(datos_crudos.columns)}\")\n",
    "    \n",
    "    # Mostrar vista previa\n",
    "    print(\"\\nüìù Vista previa de los datos:\")\n",
    "    print(datos_crudos.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cargando datos: {e}\")\n",
    "    datos_crudos = None\n",
    "\n",
    "# 2. Crear funci√≥n de preprocesamiento manual\n",
    "def preprocess_raw_data(df):\n",
    "    \"\"\"\n",
    "    Convierte datos crudos (c√≥digos) a formato procesado\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Mapeo de c√≥digos a nombres descriptivos\n",
    "    column_mapping = {\n",
    "        \"C207\": \"SEXO\",\n",
    "        \"C208\": \"EDAD\", \n",
    "        \"C366\": \"NIVEL_EDUCATIVO\",\n",
    "        \"C377\": \"ETNIA\",\n",
    "        \"C303\": \"TUVO_TRABAJO\",\n",
    "        \"C310\": \"TIPO_TRABAJADOR\",\n",
    "        \"C335\": \"BUSCA_OTRO_TRABAJO\",\n",
    "        \"OCUP300\": \"NIVEL_OCUPACION\",\n",
    "        \"C312\": \"REGISTRO_SUNAT\",\n",
    "        \"C317\": \"TAMANO_EMPRESA\",\n",
    "        \"C333\": \"QUIERE_MAS_HORAS\",\n",
    "        \"C313\": \"LIBROS_CONTABLES\",\n",
    "        \"C338\": \"FRECUENCIA_PAGO\",\n",
    "        \"ingtrabw\": \"INGRESO_PRINCIPAL\",\n",
    "        \"C375_1\": \"LIMIT_MOVIMIENTO\",\n",
    "        \"C375_2\": \"LIMIT_VISION\",\n",
    "        \"C375_3\": \"LIMIT_COMUNICACION\",\n",
    "        \"C375_4\": \"LIMIT_AUDICION\",\n",
    "        \"C375_5\": \"LIMIT_APRENDIZAJE\",\n",
    "        \"C375_6\": \"LIMIT_RELACION\",\n",
    "        \"SEGURO1\": \"SEGURO_SALUD\",\n",
    "        \"C205\": \"DIAS_AUSENTE\"\n",
    "    }\n",
    "    \n",
    "    # Renombrar columnas\n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Aplicar el mismo preprocesamiento que ya funciona\n",
    "    # Recodificar variables binarias (2 -> 0)\n",
    "    binary_cols = ['SEXO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO', 'QUIERE_MAS_HORAS', 'SEGURO_SALUD']\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({2: 0})\n",
    "    \n",
    "    # Crear variable DISCAPACIDAD\n",
    "    disability_cols = ['LIMIT_MOVIMIENTO', 'LIMIT_VISION', 'LIMIT_COMUNICACION', \n",
    "                      'LIMIT_AUDICION', 'LIMIT_APRENDIZAJE', 'LIMIT_RELACION']\n",
    "    \n",
    "    if all(col in df.columns for col in disability_cols):\n",
    "        df['DISCAPACIDAD'] = df[disability_cols].sum(axis=1).apply(lambda x: 1 if x >= 1 else 0)\n",
    "        df = df.drop(columns=disability_cols)\n",
    "    \n",
    "    # One-hot encoding\n",
    "    categorical_cols = ['ETNIA', 'TIPO_TRABAJADOR', 'NIVEL_OCUPACION', \n",
    "                       'REGISTRO_SUNAT', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True, dummy_na=True)\n",
    "    \n",
    "    # Columnas esperadas\n",
    "    expected_features = [\n",
    "        'SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO',\n",
    "        'TAMANO_EMPRESA', 'QUIERE_MAS_HORAS', 'INGRESO_PRINCIPAL', 'SEGURO_SALUD',\n",
    "        'DIAS_AUSENTE', 'DISCAPACIDAD', 'ETNIA_2', 'ETNIA_3', 'ETNIA_4', 'ETNIA_5',\n",
    "        'ETNIA_6', 'ETNIA_7', 'ETNIA_8', 'ETNIA_9', 'TIPO_TRABAJADOR_2.0',\n",
    "        'TIPO_TRABAJADOR_3.0', 'TIPO_TRABAJADOR_4.0', 'TIPO_TRABAJADOR_6.0',\n",
    "        'TIPO_TRABAJADOR_7.0', 'TIPO_TRABAJADOR_8.0', 'TIPO_TRABAJADOR_9.0',\n",
    "        'NIVEL_OCUPACION_2', 'NIVEL_OCUPACION_3', 'NIVEL_OCUPACION_4',\n",
    "        'REGISTRO_SUNAT_2.0', 'REGISTRO_SUNAT_3.0', 'LIBROS_CONTABLES_2.0',\n",
    "        'FRECUENCIA_PAGO_2.0', 'FRECUENCIA_PAGO_3.0', 'FRECUENCIA_PAGO_4.0',\n",
    "        'FRECUENCIA_PAGO_5.0'\n",
    "    ]\n",
    "    \n",
    "    # Asegurar columnas\n",
    "    for col in expected_features:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    return df[expected_features]\n",
    "\n",
    "# 3. Probar el procesamiento\n",
    "if datos_crudos is not None:\n",
    "    print(\"\\nüîß Aplicando preprocesamiento manual...\")\n",
    "    try:\n",
    "        datos_procesados = preprocess_raw_data(datos_crudos)\n",
    "        print(f\"‚úÖ Datos procesados: {datos_procesados.shape}\")\n",
    "        \n",
    "        # 4. Cargar el modelo NUEVO (kmeans_model.joblib)\n",
    "        print(\"\\nüì¶ Cargando modelo NUEVO...\")\n",
    "        modelo_nuevo = joblib.load('models/kmeans_model.joblib')\n",
    "        print(f\"‚úÖ Modelo NUEVO cargado: {type(modelo_nuevo)}\")\n",
    "        \n",
    "        # 5. Hacer predicci√≥n\n",
    "        print(\"üéØ Realizando predicci√≥n con modelo NUEVO...\")\n",
    "        clusters = modelo_nuevo.predict(datos_procesados)\n",
    "        \n",
    "        # 6. Agregar resultados\n",
    "        datos_crudos['Cluster'] = clusters\n",
    "        \n",
    "        # 7. Guardar resultados\n",
    "        output_file = 'resultados_modelo_NUEVO.csv'\n",
    "        datos_crudos.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(\"‚úÖ ¬°Predicci√≥n exitosa con MODELO NUEVO!\")\n",
    "        print(f\"üìà Distribuci√≥n de clusters:\")\n",
    "        cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "        for cluster, count in cluster_counts.items():\n",
    "            print(f\"   Cluster {cluster}: {count} registros\")\n",
    "        \n",
    "        print(f\"üíæ Resultados guardados en: {output_file}\")\n",
    "        \n",
    "        # Mostrar muestra de resultados\n",
    "        print(\"\\nüìù Muestra de resultados:\")\n",
    "        print(\"Primeras 5 filas:\")\n",
    "        display_cols = ['C207', 'C208', 'C366', 'Cluster']\n",
    "        available_display = [col for col in display_cols if col in datos_crudos.columns]\n",
    "        print(datos_crudos[available_display].head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3151a7",
   "metadata": {},
   "source": [
    "*ESCALES*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be3e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Datos originales:\n",
      "(3, 22)\n",
      "['SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'TAMANO_EMPRESA', 'INGRESO_PRINCIPAL', 'DIAS_AUSENTE', 'ETNIA', 'TIPO_TRABAJADOR', 'NIVEL_OCUPACION', 'REGISTRO_SUNAT', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO', 'QUIERE_MAS_HORAS', 'SEGURO_SALUD', 'LIMIT_MOVIMIENTO', 'LIMIT_VISION', 'LIMIT_COMUNICACION', 'LIMIT_AUDICION', 'LIMIT_APRENDIZAJE', 'LIMIT_RELACION']\n",
      "\n",
      "üîß Aplicando preprocesamiento...\n",
      "‚úÖ Datos procesados:\n",
      "Forma: (3, 36)\n",
      "Columnas: ['SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO', 'TAMANO_EMPRESA', 'QUIERE_MAS_HORAS', 'INGRESO_PRINCIPAL', 'SEGURO_SALUD', 'DIAS_AUSENTE', 'DISCAPACIDAD', 'ETNIA_2', 'ETNIA_3', 'ETNIA_4', 'ETNIA_5', 'ETNIA_6', 'ETNIA_7', 'ETNIA_8', 'ETNIA_9', 'TIPO_TRABAJADOR_2.0', 'TIPO_TRABAJADOR_3.0', 'TIPO_TRABAJADOR_4.0', 'TIPO_TRABAJADOR_6.0', 'TIPO_TRABAJADOR_7.0', 'TIPO_TRABAJADOR_8.0', 'TIPO_TRABAJADOR_9.0', 'NIVEL_OCUPACION_2', 'NIVEL_OCUPACION_3', 'NIVEL_OCUPACION_4', 'REGISTRO_SUNAT_2.0', 'REGISTRO_SUNAT_3.0', 'LIBROS_CONTABLES_2.0', 'FRECUENCIA_PAGO_2.0', 'FRECUENCIA_PAGO_3.0', 'FRECUENCIA_PAGO_4.0', 'FRECUENCIA_PAGO_5.0']\n",
      "\n",
      "üéØ Realizando predicci√≥n...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'TIPO_TRABAJADOR', 'ETNIA', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO', 'NIVEL_OCUPACION', 'REGISTRO_SUNAT'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Predecir clusters\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müéØ Realizando predicci√≥n...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatos_procesados\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClusters asignados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Mostrar resultados\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\pipeline.py:787\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 787\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1090\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1088\u001b[0m     diff \u001b[38;5;241m=\u001b[39m all_names \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[1;32m-> 1090\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     _check_n_features(\u001b[38;5;28mself\u001b[39m, X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: columns are missing: {'TIPO_TRABAJADOR', 'ETNIA', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO', 'NIVEL_OCUPACION', 'REGISTRO_SUNAT'}"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar modelo\n",
    "model = joblib.load('models/kmeans_model.joblib')\n",
    "\n",
    "# Datos crudos (formato original)\n",
    "nuevos_datos = pd.DataFrame({\n",
    "    'SEXO': [1, 2, 1],  # 1=Hombre, 2=Mujer\n",
    "    'EDAD': [25, 45, 35],  # Valores reales\n",
    "    'INGRESO_PRINCIPAL': [1500, 2500, 1800],  # Valores reales\n",
    "    'DIAS_AUSENTE': [0, 2, 1],\n",
    "    'ETNIA': ['Mestizo', 'Ind√≠gena', 'Blanco'],\n",
    "    'TIPO_TRABAJADOR': ['Empleado', 'Obrero', 'Independiente'],\n",
    "    'NIVEL_OCUPACION': ['Profesional', 'T√©cnico', 'Operario'],\n",
    "    'REGISTRO_SUNAT': [1, 2, 1],\n",
    "    'LIBROS_CONTABLES': [1, 2, 1],\n",
    "    'FRECUENCIA_PAGO': ['Mensual', 'Semanal', 'Quincenal'],\n",
    "    'TUVO_TRABAJO': [1, 2, 1],\n",
    "    'BUSCA_OTRO_TRABAJO': [1, 2, 1],\n",
    "    'QUIERE_MAS_HORAS': [1, 2, 1],\n",
    "    'SEGURO_SALUD': [1, 2, 1],\n",
    "    'LIMIT_MOVIMIENTO': [1, 2, 1],\n",
    "    'LIMIT_VISION': [1, 2, 1],\n",
    "    'LIMIT_COMUNICACION': [1, 2, 1],\n",
    "    'LIMIT_AUDICION': [1, 2, 1],\n",
    "    'LIMIT_APRENDIZAJE': [1, 2, 1],\n",
    "    'LIMIT_RELACION': [1, 2, 1]\n",
    "})\n",
    "\n",
    "# Predecir clusters\n",
    "clusters = model.predict(nuevos_datos)\n",
    "print(f\"Clusters asignados: {clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f3c88b",
   "metadata": {},
   "source": [
    "*EXTRAER DATA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee07cc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Cargando datos originales de ../data/raw/employ25_3-4-5.csv...\n",
      "üßπ Realizando limpieza b√°sica...\n",
      "üé≤ Extrayendo 25 registros aleatorios...\n",
      "üîç Seleccionando y renombrando columnas relevantes...\n",
      "üíæ Guardando muestra en new_data_sample.csv...\n",
      "\n",
      "‚úÖ Muestra creada exitosamente con nombres descriptivos!\n",
      "‚Ä¢ Total registros: 25\n",
      "‚Ä¢ Columnas: ['SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'ETNIA', 'TUVO_TRABAJO', 'TIPO_TRABAJADOR', 'BUSCA_OTRO_TRABAJO', 'NIVEL_OCUPACION', 'REGISTRO_SUNAT', 'TAMANO_EMPRESA', 'QUIERE_MAS_HORAS', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO', 'INGRESO_PRINCIPAL', 'LIMIT_MOVIMIENTO', 'LIMIT_VISION', 'LIMIT_COMUNICACION', 'LIMIT_AUDICION', 'LIMIT_APRENDIZAJE', 'LIMIT_RELACION', 'SEGURO_SALUD', 'DIAS_AUSENTE']\n",
      "\n",
      "üìù Vista previa:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEXO</th>\n",
       "      <th>EDAD</th>\n",
       "      <th>NIVEL_EDUCATIVO</th>\n",
       "      <th>ETNIA</th>\n",
       "      <th>TUVO_TRABAJO</th>\n",
       "      <th>TIPO_TRABAJADOR</th>\n",
       "      <th>BUSCA_OTRO_TRABAJO</th>\n",
       "      <th>NIVEL_OCUPACION</th>\n",
       "      <th>REGISTRO_SUNAT</th>\n",
       "      <th>TAMANO_EMPRESA</th>\n",
       "      <th>...</th>\n",
       "      <th>FRECUENCIA_PAGO</th>\n",
       "      <th>INGRESO_PRINCIPAL</th>\n",
       "      <th>LIMIT_MOVIMIENTO</th>\n",
       "      <th>LIMIT_VISION</th>\n",
       "      <th>LIMIT_COMUNICACION</th>\n",
       "      <th>LIMIT_AUDICION</th>\n",
       "      <th>LIMIT_APRENDIZAJE</th>\n",
       "      <th>LIMIT_RELACION</th>\n",
       "      <th>SEGURO_SALUD</th>\n",
       "      <th>DIAS_AUSENTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9757</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9709</th>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SEXO  EDAD  NIVEL_EDUCATIVO  ETNIA  TUVO_TRABAJO  TIPO_TRABAJADOR  \\\n",
       "9757     2    24               10      7             2              NaN   \n",
       "9709     1    78               11      1             2              NaN   \n",
       "2588     1    21                8      7             1              3.0   \n",
       "6451     2    22               10      7             2              NaN   \n",
       "518      1    43                6      7             1              3.0   \n",
       "\n",
       "      BUSCA_OTRO_TRABAJO  NIVEL_OCUPACION  REGISTRO_SUNAT  TAMANO_EMPRESA  \\\n",
       "9757                 NaN                4             NaN             NaN   \n",
       "9709                 NaN                4             NaN             NaN   \n",
       "2588                 2.0                1             2.0             3.0   \n",
       "6451                 NaN                4             NaN             NaN   \n",
       "518                  2.0                1             1.0             2.0   \n",
       "\n",
       "      ...  FRECUENCIA_PAGO  INGRESO_PRINCIPAL  LIMIT_MOVIMIENTO  LIMIT_VISION  \\\n",
       "9757  ...              NaN                NaN                 2             2   \n",
       "9709  ...              NaN                NaN                 2             2   \n",
       "2588  ...              1.0             1800.0                 2             2   \n",
       "6451  ...              NaN                NaN                 2             2   \n",
       "518   ...              1.0             4500.0                 2             2   \n",
       "\n",
       "      LIMIT_COMUNICACION  LIMIT_AUDICION  LIMIT_APRENDIZAJE  LIMIT_RELACION  \\\n",
       "9757                   2               2                  2               2   \n",
       "9709                   2               2                  2               2   \n",
       "2588                   2               2                  2               2   \n",
       "6451                   2               2                  2               2   \n",
       "518                    2               2                  2               2   \n",
       "\n",
       "      SEGURO_SALUD  DIAS_AUSENTE  \n",
       "9757             5           2.0  \n",
       "9709             1           2.0  \n",
       "2588             5           2.0  \n",
       "6451             5           2.0  \n",
       "518              6           2.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extraer_muestra.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuraci√≥n ---\n",
    "RANDOM_STATE = 42  # Para reproducibilidad\n",
    "SAMPLE_SIZE = 25   # N√∫mero de registros a extraer\n",
    "ORIGINAL_CSV = '../data/raw/employ25_3-4-5.csv'  # Ruta a tu archivo original\n",
    "OUTPUT_CSV = 'new_data_sample.csv'   # Archivo de salida\n",
    "\n",
    "# --- Mapeo de nombres (el mismo que usaste en data_loader) ---\n",
    "column_mapping = {\n",
    "    \"C207\": \"SEXO\",\n",
    "    \"C208\": \"EDAD\",\n",
    "    \"C366\": \"NIVEL_EDUCATIVO\",\n",
    "    \"C377\": \"ETNIA\",\n",
    "    \"C303\": \"TUVO_TRABAJO\",\n",
    "    \"C310\": \"TIPO_TRABAJADOR\",\n",
    "    \"C335\": \"BUSCA_OTRO_TRABAJO\",\n",
    "    \"OCUP300\": \"NIVEL_OCUPACION\",\n",
    "    \"C312\": \"REGISTRO_SUNAT\",\n",
    "    \"C317\": \"TAMANO_EMPRESA\",\n",
    "    \"C333\": \"QUIERE_MAS_HORAS\",\n",
    "    \"C313\": \"LIBROS_CONTABLES\",\n",
    "    \"C338\": \"FRECUENCIA_PAGO\",\n",
    "    \"ingtrabw\": \"INGRESO_PRINCIPAL\",\n",
    "    \"C375_1\": \"LIMIT_MOVIMIENTO\",\n",
    "    \"C375_2\": \"LIMIT_VISION\",\n",
    "    \"C375_3\": \"LIMIT_COMUNICACION\",\n",
    "    \"C375_4\": \"LIMIT_AUDICION\",\n",
    "    \"C375_5\": \"LIMIT_APRENDIZAJE\",\n",
    "    \"C375_6\": \"LIMIT_RELACION\",\n",
    "    \"SEGURO1\": \"SEGURO_SALUD\",\n",
    "    \"C205\": \"DIAS_AUSENTE\"\n",
    "}\n",
    "\n",
    "# --- 1. Cargar datos originales ---\n",
    "print(f\"üìÇ Cargando datos originales de {ORIGINAL_CSV}...\")\n",
    "df_original = pd.read_csv(ORIGINAL_CSV, na_values=[\" \"])\n",
    "\n",
    "# --- 2. Limpieza b√°sica ---\n",
    "print(\"üßπ Realizando limpieza b√°sica...\")\n",
    "# Eliminar columnas completamente vac√≠as\n",
    "df_original.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# --- 3. Extraer muestra aleatoria ---\n",
    "print(f\"üé≤ Extrayendo {SAMPLE_SIZE} registros aleatorios...\")\n",
    "sample = df_original.sample(SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "# --- 4. Seleccionar y renombrar columnas relevantes ---\n",
    "print(\"üîç Seleccionando y renombrando columnas relevantes...\")\n",
    "\n",
    "# Seleccionar solo las columnas que est√°n en el mapeo\n",
    "available_columns = [col for col in column_mapping.keys() if col in sample.columns]\n",
    "sample = sample[available_columns]\n",
    "\n",
    "# Renombrar columnas a nombres descriptivos\n",
    "sample = sample.rename(columns=column_mapping)\n",
    "\n",
    "# --- 5. Guardar muestra ---\n",
    "print(f\"üíæ Guardando muestra en {OUTPUT_CSV}...\")\n",
    "sample.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# --- 6. Resultado final ---\n",
    "print(\"\\n‚úÖ Muestra creada exitosamente con nombres descriptivos!\")\n",
    "print(f\"‚Ä¢ Total registros: {len(sample)}\")\n",
    "print(f\"‚Ä¢ Columnas: {sample.columns.tolist()}\")\n",
    "print(\"\\nüìù Vista previa:\")\n",
    "display(sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6401c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Cargando datos originales de ../data/raw/employ25_3-4-5.csv...\n",
      "üßπ Realizando limpieza b√°sica...\n",
      "üé≤ Extrayendo 25 registros aleatorios...\n",
      "üîç Seleccionando columnas relevantes...\n",
      "üíæ Guardando muestra en new_data_sample_SINP.csv...\n",
      "\n",
      "‚úÖ Muestra creada exitosamente!\n",
      "‚Ä¢ Total registros: 25\n",
      "‚Ä¢ Columnas: ['C207', 'C208', 'C366', 'C377', 'C303', 'C310', 'C335', 'OCUP300', 'C312', 'C317', 'C333', 'C313', 'C338', 'ingtrabw', 'C375_1', 'C375_2', 'C375_3', 'C375_4', 'C375_5', 'C375_6', 'SEGURO1', 'C205']\n",
      "\n",
      "üìù Vista previa:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C207</th>\n",
       "      <th>C208</th>\n",
       "      <th>C366</th>\n",
       "      <th>C377</th>\n",
       "      <th>C303</th>\n",
       "      <th>C310</th>\n",
       "      <th>C335</th>\n",
       "      <th>OCUP300</th>\n",
       "      <th>C312</th>\n",
       "      <th>C317</th>\n",
       "      <th>...</th>\n",
       "      <th>C338</th>\n",
       "      <th>ingtrabw</th>\n",
       "      <th>C375_1</th>\n",
       "      <th>C375_2</th>\n",
       "      <th>C375_3</th>\n",
       "      <th>C375_4</th>\n",
       "      <th>C375_5</th>\n",
       "      <th>C375_6</th>\n",
       "      <th>SEGURO1</th>\n",
       "      <th>C205</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9757</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9709</th>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      C207  C208  C366  C377  C303  C310  C335  OCUP300  C312  C317  ...  \\\n",
       "9757     2    24    10     7     2   NaN   NaN        4   NaN   NaN  ...   \n",
       "9709     1    78    11     1     2   NaN   NaN        4   NaN   NaN  ...   \n",
       "2588     1    21     8     7     1   3.0   2.0        1   2.0   3.0  ...   \n",
       "6451     2    22    10     7     2   NaN   NaN        4   NaN   NaN  ...   \n",
       "518      1    43     6     7     1   3.0   2.0        1   1.0   2.0  ...   \n",
       "\n",
       "      C338  ingtrabw  C375_1  C375_2  C375_3  C375_4  C375_5  C375_6  SEGURO1  \\\n",
       "9757   NaN       NaN       2       2       2       2       2       2        5   \n",
       "9709   NaN       NaN       2       2       2       2       2       2        1   \n",
       "2588   1.0    1800.0       2       2       2       2       2       2        5   \n",
       "6451   NaN       NaN       2       2       2       2       2       2        5   \n",
       "518    1.0    4500.0       2       2       2       2       2       2        6   \n",
       "\n",
       "      C205  \n",
       "9757   2.0  \n",
       "9709   2.0  \n",
       "2588   2.0  \n",
       "6451   2.0  \n",
       "518    2.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extraer_muestra.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuraci√≥n ---\n",
    "RANDOM_STATE = 42  # Para reproducibilidad\n",
    "SAMPLE_SIZE = 25   # N√∫mero de registros a extraer\n",
    "ORIGINAL_CSV = '../data/raw/employ25_3-4-5.csv'  # Ruta a tu archivo original\n",
    "OUTPUT_CSV = 'new_data_sample_SINP.csv'   # Archivo de salida\n",
    "\n",
    "# --- 1. Cargar datos originales ---\n",
    "print(f\"üìÇ Cargando datos originales de {ORIGINAL_CSV}...\")\n",
    "df_original = pd.read_csv(ORIGINAL_CSV, na_values=[\" \"])\n",
    "\n",
    "# --- 2. Limpieza b√°sica ---\n",
    "print(\"üßπ Realizando limpieza b√°sica...\")\n",
    "# Eliminar columnas completamente vac√≠as\n",
    "df_original.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# --- 3. Extraer muestra aleatoria ---\n",
    "print(f\"üé≤ Extrayendo {SAMPLE_SIZE} registros aleatorios...\")\n",
    "sample = df_original.sample(SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "# --- 4. Seleccionar columnas relevantes ---\n",
    "# Las mismas columnas que usaste en tu proyecto original\n",
    "relevant_columns = [\n",
    "    \"C207\", \"C208\", \"C366\", \"C377\", \"C303\", \"C310\", \n",
    "    \"C335\", \"OCUP300\", \"C312\", \"C317\", \"C333\", \n",
    "    \"C313\", \"C338\", \"ingtrabw\", \"C375_1\", \"C375_2\", \n",
    "    \"C375_3\", \"C375_4\", \"C375_5\", \"C375_6\", \"SEGURO1\", \"C205\"\n",
    "]\n",
    "\n",
    "print(\"üîç Seleccionando columnas relevantes...\")\n",
    "sample = sample[relevant_columns]\n",
    "\n",
    "# --- 5. Guardar muestra ---\n",
    "print(f\"üíæ Guardando muestra en {OUTPUT_CSV}...\")\n",
    "sample.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# --- 6. Resultado final ---\n",
    "print(\"\\n‚úÖ Muestra creada exitosamente!\")\n",
    "print(f\"‚Ä¢ Total registros: {len(sample)}\")\n",
    "print(f\"‚Ä¢ Columnas: {sample.columns.tolist()}\")\n",
    "print(\"\\nüìù Vista previa:\")\n",
    "display(sample.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
