{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac85b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Cargando modelo...\n",
      "📊 Cargando datos...\n",
      "🔧 Aplicando preprocesamiento...\n",
      "✅ Datos procesados: (25, 36)\n",
      "📋 Columnas procesadas: ['SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO', 'TAMANO_EMPRESA', 'QUIERE_MAS_HORAS', 'INGRESO_PRINCIPAL', 'SEGURO_SALUD', 'DIAS_AUSENTE', 'DISCAPACIDAD', 'ETNIA_2', 'ETNIA_3', 'ETNIA_4', 'ETNIA_5', 'ETNIA_6', 'ETNIA_7', 'ETNIA_8', 'ETNIA_9', 'TIPO_TRABAJADOR_2.0', 'TIPO_TRABAJADOR_3.0', 'TIPO_TRABAJADOR_4.0', 'TIPO_TRABAJADOR_6.0', 'TIPO_TRABAJADOR_7.0', 'TIPO_TRABAJADOR_8.0', 'TIPO_TRABAJADOR_9.0', 'NIVEL_OCUPACION_2', 'NIVEL_OCUPACION_3', 'NIVEL_OCUPACION_4', 'REGISTRO_SUNAT_2.0', 'REGISTRO_SUNAT_3.0', 'LIBROS_CONTABLES_2.0', 'FRECUENCIA_PAGO_2.0', 'FRECUENCIA_PAGO_3.0', 'FRECUENCIA_PAGO_4.0', 'FRECUENCIA_PAGO_5.0']\n",
      "🎯 Realizando predicción...\n",
      "✅ ¡Predicción completada exitosamente!\n",
      "📈 Distribución de clusters:\n",
      "   Cluster 0: 10 registros\n",
      "   Cluster 1: 14 registros\n",
      "   Cluster 2: 1 registros\n",
      "💾 Resultados guardados en: resultados_con_clusters.csv\n",
      "\n",
      "📝 Muestra de resultados:\n",
      "   SEXO  EDAD  NIVEL_EDUCATIVO  Cluster\n",
      "0     2    24               10        1\n",
      "1     1    78               11        1\n",
      "2     1    21                8        0\n",
      "3     2    22               10        1\n",
      "4     1    43                6        0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# 1. Cargar el modelo\n",
    "print(\"📦 Cargando modelo...\")\n",
    "pipeline = joblib.load('../models/kmeans_model__oldc.joblib')\n",
    "\n",
    "# 2. Cargar nuevos datos\n",
    "print(\"📊 Cargando datos...\")\n",
    "new_data = pd.read_csv('new_data_sample.csv')\n",
    "\n",
    "# 3. Función de preprocesamiento (debe coincidir exactamente con el entrenamiento)\n",
    "def preprocess_for_prediction(df):\n",
    "    \"\"\"\n",
    "    Aplica el mismo preprocesamiento que se usó durante el entrenamiento\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Recodificar variables binarias (2 -> 0)\n",
    "    binary_cols = ['SEXO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO', 'QUIERE_MAS_HORAS', 'SEGURO_SALUD']\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({2: 0})\n",
    "    \n",
    "    # Crear variable DISCAPACIDAD a partir de las limitaciones\n",
    "    disability_cols = ['LIMIT_MOVIMIENTO', 'LIMIT_VISION', 'LIMIT_COMUNICACION', \n",
    "                      'LIMIT_AUDICION', 'LIMIT_APRENDIZAJE', 'LIMIT_RELACION']\n",
    "    \n",
    "    if all(col in df.columns for col in disability_cols):\n",
    "        df['DISCAPACIDAD'] = df[disability_cols].sum(axis=1).apply(lambda x: 1 if x >= 1 else 0)\n",
    "        df = df.drop(columns=disability_cols)\n",
    "    \n",
    "    # One-hot encoding para variables categóricas\n",
    "    categorical_cols = ['ETNIA', 'TIPO_TRABAJADOR', 'NIVEL_OCUPACION', \n",
    "                       'REGISTRO_SUNAT', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True, dummy_na=True)\n",
    "    \n",
    "    # Columnas que el modelo espera (basado en el modelo cargado)\n",
    "    expected_features = [\n",
    "        'SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO',\n",
    "        'TAMANO_EMPRESA', 'QUIERE_MAS_HORAS', 'INGRESO_PRINCIPAL', 'SEGURO_SALUD',\n",
    "        'DIAS_AUSENTE', 'DISCAPACIDAD', 'ETNIA_2', 'ETNIA_3', 'ETNIA_4', 'ETNIA_5',\n",
    "        'ETNIA_6', 'ETNIA_7', 'ETNIA_8', 'ETNIA_9', 'TIPO_TRABAJADOR_2.0',\n",
    "        'TIPO_TRABAJADOR_3.0', 'TIPO_TRABAJADOR_4.0', 'TIPO_TRABAJADOR_6.0',\n",
    "        'TIPO_TRABAJADOR_7.0', 'TIPO_TRABAJADOR_8.0', 'TIPO_TRABAJADOR_9.0',\n",
    "        'NIVEL_OCUPACION_2', 'NIVEL_OCUPACION_3', 'NIVEL_OCUPACION_4',\n",
    "        'REGISTRO_SUNAT_2.0', 'REGISTRO_SUNAT_3.0', 'LIBROS_CONTABLES_2.0',\n",
    "        'FRECUENCIA_PAGO_2.0', 'FRECUENCIA_PAGO_3.0', 'FRECUENCIA_PAGO_4.0',\n",
    "        'FRECUENCIA_PAGO_5.0'\n",
    "    ]\n",
    "    \n",
    "    # Asegurar que todas las columnas esperadas estén presentes\n",
    "    for col in expected_features:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    # Retornar solo las columnas en el orden correcto\n",
    "    return df[expected_features]\n",
    "\n",
    "# 4. Aplicar preprocesamiento\n",
    "print(\"🔧 Aplicando preprocesamiento...\")\n",
    "try:\n",
    "    processed_data = preprocess_for_prediction(new_data)\n",
    "    print(f\"✅ Datos procesados: {processed_data.shape}\")\n",
    "    print(f\"📋 Columnas procesadas: {list(processed_data.columns)}\")\n",
    "    \n",
    "    # 5. Hacer predicción\n",
    "    print(\"🎯 Realizando predicción...\")\n",
    "    clusters = pipeline.predict(processed_data)\n",
    "    \n",
    "    # 6. Agregar resultados a los datos originales\n",
    "    new_data['Cluster'] = clusters\n",
    "    \n",
    "    # 7. Guardar resultados\n",
    "    output_file = 'resultados_con_clusters.csv'\n",
    "    new_data.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"✅ ¡Predicción completada exitosamente!\")\n",
    "    print(f\"📈 Distribución de clusters:\")\n",
    "    cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        print(f\"   Cluster {cluster}: {count} registros\")\n",
    "    \n",
    "    print(f\"💾 Resultados guardados en: {output_file}\")\n",
    "    \n",
    "    # Mostrar una muestra de los resultados\n",
    "    print(\"\\n📝 Muestra de resultados:\")\n",
    "    print(new_data[['SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'Cluster']].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"🚨 ERROR: {e}\")\n",
    "    print(\"\\n🔍 Información de depuración:\")\n",
    "    print(f\"Forma de los datos originales: {new_data.shape}\")\n",
    "    print(f\"Columnas disponibles: {list(new_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "568fb0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Cargando modelo...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 1. Cargar el modelo\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📦 Cargando modelo...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../models/kmeans_model.joblib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2. Cargar nuevos datos\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Cargando datos...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\numpy_pickle.py:749\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[0;32m    744\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m    746\u001b[0m             \u001b[38;5;66;03m# A memory-mapped array has to be mapped with the endianness\u001b[39;00m\n\u001b[0;32m    747\u001b[0m             \u001b[38;5;66;03m# it has been written with. Other arrays are coerced to the\u001b[39;00m\n\u001b[0;32m    748\u001b[0m             \u001b[38;5;66;03m# native endianness of the host system.\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m                \u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidated_mmap_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\numpy_pickle.py:626\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    624\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 626\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    628\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    630\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    633\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    634\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pickle.py:1255\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1255\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pickle.py:1580\u001b[0m, in \u001b[0;36m_Unpickler.load_stack_global\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m   1579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTACK_GLOBAL requires str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pickle.py:1621\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m   1619\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING:\n\u001b[0;32m   1620\u001b[0m         module \u001b[38;5;241m=\u001b[39m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING[module]\n\u001b[1;32m-> 1621\u001b[0m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m   1623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _getattribute(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pipeline'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pipeline import \n",
    "\n",
    "\n",
    "# 1. Cargar el modelo\n",
    "print(\"📦 Cargando modelo...\")\n",
    "pipeline = joblib.load('../models/kmeans_model.joblib')\n",
    "\n",
    "# 2. Cargar nuevos datos\n",
    "print(\"📊 Cargando datos...\")\n",
    "new_data = pd.read_csv('new_data_sample.csv')\n",
    "\n",
    "# 3. Función de preprocesamiento (debe coincidir exactamente con el entrenamiento)\n",
    "def preprocess_for_prediction(df):\n",
    "    \"\"\"\n",
    "    Aplica el mismo preprocesamiento que se usó durante el entrenamiento\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Recodificar variables binarias (2 -> 0)\n",
    "    binary_cols = ['SEXO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO', 'QUIERE_MAS_HORAS', 'SEGURO_SALUD']\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({2: 0})\n",
    "    \n",
    "    # Crear variable DISCAPACIDAD a partir de las limitaciones\n",
    "    disability_cols = ['LIMIT_MOVIMIENTO', 'LIMIT_VISION', 'LIMIT_COMUNICACION', \n",
    "                      'LIMIT_AUDICION', 'LIMIT_APRENDIZAJE', 'LIMIT_RELACION']\n",
    "    \n",
    "    if all(col in df.columns for col in disability_cols):\n",
    "        df['DISCAPACIDAD'] = df[disability_cols].sum(axis=1).apply(lambda x: 1 if x >= 1 else 0)\n",
    "        df = df.drop(columns=disability_cols)\n",
    "    \n",
    "    # One-hot encoding para variables categóricas\n",
    "    categorical_cols = ['ETNIA', 'TIPO_TRABAJADOR', 'NIVEL_OCUPACION', \n",
    "                       'REGISTRO_SUNAT', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True, dummy_na=True)\n",
    "    \n",
    "    # Columnas que el modelo espera (basado en el modelo cargado)\n",
    "    expected_features = [\n",
    "        'SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO',\n",
    "        'TAMANO_EMPRESA', 'QUIERE_MAS_HORAS', 'INGRESO_PRINCIPAL', 'SEGURO_SALUD',\n",
    "        'DIAS_AUSENTE', 'DISCAPACIDAD', 'ETNIA_2', 'ETNIA_3', 'ETNIA_4', 'ETNIA_5',\n",
    "        'ETNIA_6', 'ETNIA_7', 'ETNIA_8', 'ETNIA_9', 'TIPO_TRABAJADOR_2.0',\n",
    "        'TIPO_TRABAJADOR_3.0', 'TIPO_TRABAJADOR_4.0', 'TIPO_TRABAJADOR_6.0',\n",
    "        'TIPO_TRABAJADOR_7.0', 'TIPO_TRABAJADOR_8.0', 'TIPO_TRABAJADOR_9.0',\n",
    "        'NIVEL_OCUPACION_2', 'NIVEL_OCUPACION_3', 'NIVEL_OCUPACION_4',\n",
    "        'REGISTRO_SUNAT_2.0', 'REGISTRO_SUNAT_3.0', 'LIBROS_CONTABLES_2.0',\n",
    "        'FRECUENCIA_PAGO_2.0', 'FRECUENCIA_PAGO_3.0', 'FRECUENCIA_PAGO_4.0',\n",
    "        'FRECUENCIA_PAGO_5.0'\n",
    "    ]\n",
    "    \n",
    "    # Asegurar que todas las columnas esperadas estén presentes\n",
    "    for col in expected_features:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    # Retornar solo las columnas en el orden correcto\n",
    "    return df[expected_features]\n",
    "\n",
    "# 4. Aplicar preprocesamiento\n",
    "print(\"🔧 Aplicando preprocesamiento...\")\n",
    "try:\n",
    "    processed_data = preprocess_for_prediction(new_data)\n",
    "    print(f\"✅ Datos procesados: {processed_data.shape}\")\n",
    "    print(f\"📋 Columnas procesadas: {list(processed_data.columns)}\")\n",
    "    \n",
    "    # 5. Hacer predicción\n",
    "    print(\"🎯 Realizando predicción...\")\n",
    "    clusters = pipeline.predict(processed_data)\n",
    "    \n",
    "    # 6. Agregar resultados a los datos originales\n",
    "    new_data['Cluster'] = clusters\n",
    "    \n",
    "    # 7. Guardar resultados\n",
    "    output_file = 'resultados_con_clusters.csv'\n",
    "    new_data.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"✅ ¡Predicción completada exitosamente!\")\n",
    "    print(f\"📈 Distribución de clusters:\")\n",
    "    cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        print(f\"   Cluster {cluster}: {count} registros\")\n",
    "    \n",
    "    print(f\"💾 Resultados guardados en: {output_file}\")\n",
    "    \n",
    "    # Mostrar una muestra de los resultados\n",
    "    print(\"\\n📝 Muestra de resultados:\")\n",
    "    print(new_data[['SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'Cluster']].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"🚨 ERROR: {e}\")\n",
    "    print(\"\\n🔍 Información de depuración:\")\n",
    "    print(f\"Forma de los datos originales: {new_data.shape}\")\n",
    "    print(f\"Columnas disponibles: {list(new_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a708ea3e",
   "metadata": {},
   "source": [
    "NUEVO MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10139e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🤖 USANDO EL MODELO NUEVO - kmeans_model.joblib\n",
      "============================================================\n",
      "📊 Cargando datos sin procesar...\n",
      "✅ Datos crudos cargados: (25, 22)\n",
      "📋 Columnas disponibles: ['C207', 'C208', 'C366', 'C377', 'C303', 'C310', 'C335', 'OCUP300', 'C312', 'C317', 'C333', 'C313', 'C338', 'ingtrabw', 'C375_1', 'C375_2', 'C375_3', 'C375_4', 'C375_5', 'C375_6', 'SEGURO1', 'C205']\n",
      "\n",
      "📝 Vista previa de los datos:\n",
      "   C207  C208  C366  C377  C303  C310  C335  OCUP300  C312  C317  ...  C338  \\\n",
      "0     2    24    10     7     2   NaN   NaN        4   NaN   NaN  ...   NaN   \n",
      "1     1    78    11     1     2   NaN   NaN        4   NaN   NaN  ...   NaN   \n",
      "2     1    21     8     7     1   3.0   2.0        1   2.0   3.0  ...   1.0   \n",
      "3     2    22    10     7     2   NaN   NaN        4   NaN   NaN  ...   NaN   \n",
      "4     1    43     6     7     1   3.0   2.0        1   1.0   2.0  ...   1.0   \n",
      "\n",
      "   ingtrabw  C375_1  C375_2  C375_3  C375_4  C375_5  C375_6  SEGURO1  C205  \n",
      "0       NaN       2       2       2       2       2       2        5   2.0  \n",
      "1       NaN       2       2       2       2       2       2        1   2.0  \n",
      "2    1800.0       2       2       2       2       2       2        5   2.0  \n",
      "3       NaN       2       2       2       2       2       2        5   2.0  \n",
      "4    4500.0       2       2       2       2       2       2        6   2.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "🔧 Aplicando preprocesamiento manual...\n",
      "✅ Datos procesados: (25, 36)\n",
      "\n",
      "📦 Cargando modelo NUEVO...\n",
      "✅ Modelo NUEVO cargado: <class 'sklearn.pipeline.Pipeline'>\n",
      "🎯 Realizando predicción con modelo NUEVO...\n",
      "❌ Error: columns are missing: {'TIPO_TRABAJADOR', 'ETNIA', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO', 'NIVEL_OCUPACION', 'REGISTRO_SUNAT'}\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\destr\\AppData\\Local\\Temp\\ipykernel_14648\\3589152920.py\", line 127, in <module>\n",
      "    clusters = modelo_nuevo.predict(datos_procesados)\n",
      "  File \"c:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\pipeline.py\", line 787, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"c:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"c:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1090, in transform\n",
      "    raise ValueError(f\"columns are missing: {diff}\")\n",
      "ValueError: columns are missing: {'TIPO_TRABAJADOR', 'ETNIA', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO', 'NIVEL_OCUPACION', 'REGISTRO_SUNAT'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # PATH funcionando ✅\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"🤖 USANDO EL MODELO NUEVO - kmeans_model.joblib\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Cargar datos sin procesar\n",
    "print(\"📊 Cargando datos sin procesar...\")\n",
    "try:\n",
    "    datos_crudos = pd.read_csv('notebooks/new_data_sample_SINP.csv', na_values=[\" \"])\n",
    "    \n",
    "    print(f\"✅ Datos crudos cargados: {datos_crudos.shape}\")\n",
    "    print(f\"📋 Columnas disponibles: {list(datos_crudos.columns)}\")\n",
    "    \n",
    "    # Mostrar vista previa\n",
    "    print(\"\\n📝 Vista previa de los datos:\")\n",
    "    print(datos_crudos.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error cargando datos: {e}\")\n",
    "    datos_crudos = None\n",
    "\n",
    "# 2. Crear función de preprocesamiento manual\n",
    "def preprocess_raw_data(df):\n",
    "    \"\"\"\n",
    "    Convierte datos crudos (códigos) a formato procesado\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Mapeo de códigos a nombres descriptivos\n",
    "    column_mapping = {\n",
    "        \"C207\": \"SEXO\",\n",
    "        \"C208\": \"EDAD\", \n",
    "        \"C366\": \"NIVEL_EDUCATIVO\",\n",
    "        \"C377\": \"ETNIA\",\n",
    "        \"C303\": \"TUVO_TRABAJO\",\n",
    "        \"C310\": \"TIPO_TRABAJADOR\",\n",
    "        \"C335\": \"BUSCA_OTRO_TRABAJO\",\n",
    "        \"OCUP300\": \"NIVEL_OCUPACION\",\n",
    "        \"C312\": \"REGISTRO_SUNAT\",\n",
    "        \"C317\": \"TAMANO_EMPRESA\",\n",
    "        \"C333\": \"QUIERE_MAS_HORAS\",\n",
    "        \"C313\": \"LIBROS_CONTABLES\",\n",
    "        \"C338\": \"FRECUENCIA_PAGO\",\n",
    "        \"ingtrabw\": \"INGRESO_PRINCIPAL\",\n",
    "        \"C375_1\": \"LIMIT_MOVIMIENTO\",\n",
    "        \"C375_2\": \"LIMIT_VISION\",\n",
    "        \"C375_3\": \"LIMIT_COMUNICACION\",\n",
    "        \"C375_4\": \"LIMIT_AUDICION\",\n",
    "        \"C375_5\": \"LIMIT_APRENDIZAJE\",\n",
    "        \"C375_6\": \"LIMIT_RELACION\",\n",
    "        \"SEGURO1\": \"SEGURO_SALUD\",\n",
    "        \"C205\": \"DIAS_AUSENTE\"\n",
    "    }\n",
    "    \n",
    "    # Renombrar columnas\n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Aplicar el mismo preprocesamiento que ya funciona\n",
    "    # Recodificar variables binarias (2 -> 0)\n",
    "    binary_cols = ['SEXO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO', 'QUIERE_MAS_HORAS', 'SEGURO_SALUD']\n",
    "    for col in binary_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({2: 0})\n",
    "    \n",
    "    # Crear variable DISCAPACIDAD\n",
    "    disability_cols = ['LIMIT_MOVIMIENTO', 'LIMIT_VISION', 'LIMIT_COMUNICACION', \n",
    "                      'LIMIT_AUDICION', 'LIMIT_APRENDIZAJE', 'LIMIT_RELACION']\n",
    "    \n",
    "    if all(col in df.columns for col in disability_cols):\n",
    "        df['DISCAPACIDAD'] = df[disability_cols].sum(axis=1).apply(lambda x: 1 if x >= 1 else 0)\n",
    "        df = df.drop(columns=disability_cols)\n",
    "    \n",
    "    # One-hot encoding\n",
    "    categorical_cols = ['ETNIA', 'TIPO_TRABAJADOR', 'NIVEL_OCUPACION', \n",
    "                       'REGISTRO_SUNAT', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True, dummy_na=True)\n",
    "    \n",
    "    # Columnas esperadas\n",
    "    expected_features = [\n",
    "        'SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO',\n",
    "        'TAMANO_EMPRESA', 'QUIERE_MAS_HORAS', 'INGRESO_PRINCIPAL', 'SEGURO_SALUD',\n",
    "        'DIAS_AUSENTE', 'DISCAPACIDAD', 'ETNIA_2', 'ETNIA_3', 'ETNIA_4', 'ETNIA_5',\n",
    "        'ETNIA_6', 'ETNIA_7', 'ETNIA_8', 'ETNIA_9', 'TIPO_TRABAJADOR_2.0',\n",
    "        'TIPO_TRABAJADOR_3.0', 'TIPO_TRABAJADOR_4.0', 'TIPO_TRABAJADOR_6.0',\n",
    "        'TIPO_TRABAJADOR_7.0', 'TIPO_TRABAJADOR_8.0', 'TIPO_TRABAJADOR_9.0',\n",
    "        'NIVEL_OCUPACION_2', 'NIVEL_OCUPACION_3', 'NIVEL_OCUPACION_4',\n",
    "        'REGISTRO_SUNAT_2.0', 'REGISTRO_SUNAT_3.0', 'LIBROS_CONTABLES_2.0',\n",
    "        'FRECUENCIA_PAGO_2.0', 'FRECUENCIA_PAGO_3.0', 'FRECUENCIA_PAGO_4.0',\n",
    "        'FRECUENCIA_PAGO_5.0'\n",
    "    ]\n",
    "    \n",
    "    # Asegurar columnas\n",
    "    for col in expected_features:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    return df[expected_features]\n",
    "\n",
    "# 3. Probar el procesamiento\n",
    "if datos_crudos is not None:\n",
    "    print(\"\\n🔧 Aplicando preprocesamiento manual...\")\n",
    "    try:\n",
    "        datos_procesados = preprocess_raw_data(datos_crudos)\n",
    "        print(f\"✅ Datos procesados: {datos_procesados.shape}\")\n",
    "        \n",
    "        # 4. Cargar el modelo NUEVO (kmeans_model.joblib)\n",
    "        print(\"\\n📦 Cargando modelo NUEVO...\")\n",
    "        modelo_nuevo = joblib.load('models/kmeans_model.joblib')\n",
    "        print(f\"✅ Modelo NUEVO cargado: {type(modelo_nuevo)}\")\n",
    "        \n",
    "        # 5. Hacer predicción\n",
    "        print(\"🎯 Realizando predicción con modelo NUEVO...\")\n",
    "        clusters = modelo_nuevo.predict(datos_procesados)\n",
    "        \n",
    "        # 6. Agregar resultados\n",
    "        datos_crudos['Cluster'] = clusters\n",
    "        \n",
    "        # 7. Guardar resultados\n",
    "        output_file = 'resultados_modelo_NUEVO.csv'\n",
    "        datos_crudos.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(\"✅ ¡Predicción exitosa con MODELO NUEVO!\")\n",
    "        print(f\"📈 Distribución de clusters:\")\n",
    "        cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "        for cluster, count in cluster_counts.items():\n",
    "            print(f\"   Cluster {cluster}: {count} registros\")\n",
    "        \n",
    "        print(f\"💾 Resultados guardados en: {output_file}\")\n",
    "        \n",
    "        # Mostrar muestra de resultados\n",
    "        print(\"\\n📝 Muestra de resultados:\")\n",
    "        print(\"Primeras 5 filas:\")\n",
    "        display_cols = ['C207', 'C208', 'C366', 'Cluster']\n",
    "        available_display = [col for col in display_cols if col in datos_crudos.columns]\n",
    "        print(datos_crudos[available_display].head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3151a7",
   "metadata": {},
   "source": [
    "*ESCALES*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be3e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Datos originales:\n",
      "(3, 22)\n",
      "['SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'TAMANO_EMPRESA', 'INGRESO_PRINCIPAL', 'DIAS_AUSENTE', 'ETNIA', 'TIPO_TRABAJADOR', 'NIVEL_OCUPACION', 'REGISTRO_SUNAT', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO', 'QUIERE_MAS_HORAS', 'SEGURO_SALUD', 'LIMIT_MOVIMIENTO', 'LIMIT_VISION', 'LIMIT_COMUNICACION', 'LIMIT_AUDICION', 'LIMIT_APRENDIZAJE', 'LIMIT_RELACION']\n",
      "\n",
      "🔧 Aplicando preprocesamiento...\n",
      "✅ Datos procesados:\n",
      "Forma: (3, 36)\n",
      "Columnas: ['SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'TUVO_TRABAJO', 'BUSCA_OTRO_TRABAJO', 'TAMANO_EMPRESA', 'QUIERE_MAS_HORAS', 'INGRESO_PRINCIPAL', 'SEGURO_SALUD', 'DIAS_AUSENTE', 'DISCAPACIDAD', 'ETNIA_2', 'ETNIA_3', 'ETNIA_4', 'ETNIA_5', 'ETNIA_6', 'ETNIA_7', 'ETNIA_8', 'ETNIA_9', 'TIPO_TRABAJADOR_2.0', 'TIPO_TRABAJADOR_3.0', 'TIPO_TRABAJADOR_4.0', 'TIPO_TRABAJADOR_6.0', 'TIPO_TRABAJADOR_7.0', 'TIPO_TRABAJADOR_8.0', 'TIPO_TRABAJADOR_9.0', 'NIVEL_OCUPACION_2', 'NIVEL_OCUPACION_3', 'NIVEL_OCUPACION_4', 'REGISTRO_SUNAT_2.0', 'REGISTRO_SUNAT_3.0', 'LIBROS_CONTABLES_2.0', 'FRECUENCIA_PAGO_2.0', 'FRECUENCIA_PAGO_3.0', 'FRECUENCIA_PAGO_4.0', 'FRECUENCIA_PAGO_5.0']\n",
      "\n",
      "🎯 Realizando predicción...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'TIPO_TRABAJADOR', 'ETNIA', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO', 'NIVEL_OCUPACION', 'REGISTRO_SUNAT'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Predecir clusters\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎯 Realizando predicción...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatos_procesados\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClusters asignados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Mostrar resultados\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\pipeline.py:787\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 787\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\destr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1090\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1088\u001b[0m     diff \u001b[38;5;241m=\u001b[39m all_names \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[1;32m-> 1090\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     _check_n_features(\u001b[38;5;28mself\u001b[39m, X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: columns are missing: {'TIPO_TRABAJADOR', 'ETNIA', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO', 'NIVEL_OCUPACION', 'REGISTRO_SUNAT'}"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar modelo\n",
    "model = joblib.load('models/kmeans_model.joblib')\n",
    "\n",
    "# Datos crudos (formato original)\n",
    "nuevos_datos = pd.DataFrame({\n",
    "    'SEXO': [1, 2, 1],  # 1=Hombre, 2=Mujer\n",
    "    'EDAD': [25, 45, 35],  # Valores reales\n",
    "    'INGRESO_PRINCIPAL': [1500, 2500, 1800],  # Valores reales\n",
    "    'DIAS_AUSENTE': [0, 2, 1],\n",
    "    'ETNIA': ['Mestizo', 'Indígena', 'Blanco'],\n",
    "    'TIPO_TRABAJADOR': ['Empleado', 'Obrero', 'Independiente'],\n",
    "    'NIVEL_OCUPACION': ['Profesional', 'Técnico', 'Operario'],\n",
    "    'REGISTRO_SUNAT': [1, 2, 1],\n",
    "    'LIBROS_CONTABLES': [1, 2, 1],\n",
    "    'FRECUENCIA_PAGO': ['Mensual', 'Semanal', 'Quincenal'],\n",
    "    'TUVO_TRABAJO': [1, 2, 1],\n",
    "    'BUSCA_OTRO_TRABAJO': [1, 2, 1],\n",
    "    'QUIERE_MAS_HORAS': [1, 2, 1],\n",
    "    'SEGURO_SALUD': [1, 2, 1],\n",
    "    'LIMIT_MOVIMIENTO': [1, 2, 1],\n",
    "    'LIMIT_VISION': [1, 2, 1],\n",
    "    'LIMIT_COMUNICACION': [1, 2, 1],\n",
    "    'LIMIT_AUDICION': [1, 2, 1],\n",
    "    'LIMIT_APRENDIZAJE': [1, 2, 1],\n",
    "    'LIMIT_RELACION': [1, 2, 1]\n",
    "})\n",
    "\n",
    "# Predecir clusters\n",
    "clusters = model.predict(nuevos_datos)\n",
    "print(f\"Clusters asignados: {clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f3c88b",
   "metadata": {},
   "source": [
    "*EXTRAER DATA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee07cc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Cargando datos originales de ../data/raw/employ25_3-4-5.csv...\n",
      "🧹 Realizando limpieza básica...\n",
      "🎲 Extrayendo 25 registros aleatorios...\n",
      "🔍 Seleccionando y renombrando columnas relevantes...\n",
      "💾 Guardando muestra en new_data_sample.csv...\n",
      "\n",
      "✅ Muestra creada exitosamente con nombres descriptivos!\n",
      "• Total registros: 25\n",
      "• Columnas: ['SEXO', 'EDAD', 'NIVEL_EDUCATIVO', 'ETNIA', 'TUVO_TRABAJO', 'TIPO_TRABAJADOR', 'BUSCA_OTRO_TRABAJO', 'NIVEL_OCUPACION', 'REGISTRO_SUNAT', 'TAMANO_EMPRESA', 'QUIERE_MAS_HORAS', 'LIBROS_CONTABLES', 'FRECUENCIA_PAGO', 'INGRESO_PRINCIPAL', 'LIMIT_MOVIMIENTO', 'LIMIT_VISION', 'LIMIT_COMUNICACION', 'LIMIT_AUDICION', 'LIMIT_APRENDIZAJE', 'LIMIT_RELACION', 'SEGURO_SALUD', 'DIAS_AUSENTE']\n",
      "\n",
      "📝 Vista previa:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEXO</th>\n",
       "      <th>EDAD</th>\n",
       "      <th>NIVEL_EDUCATIVO</th>\n",
       "      <th>ETNIA</th>\n",
       "      <th>TUVO_TRABAJO</th>\n",
       "      <th>TIPO_TRABAJADOR</th>\n",
       "      <th>BUSCA_OTRO_TRABAJO</th>\n",
       "      <th>NIVEL_OCUPACION</th>\n",
       "      <th>REGISTRO_SUNAT</th>\n",
       "      <th>TAMANO_EMPRESA</th>\n",
       "      <th>...</th>\n",
       "      <th>FRECUENCIA_PAGO</th>\n",
       "      <th>INGRESO_PRINCIPAL</th>\n",
       "      <th>LIMIT_MOVIMIENTO</th>\n",
       "      <th>LIMIT_VISION</th>\n",
       "      <th>LIMIT_COMUNICACION</th>\n",
       "      <th>LIMIT_AUDICION</th>\n",
       "      <th>LIMIT_APRENDIZAJE</th>\n",
       "      <th>LIMIT_RELACION</th>\n",
       "      <th>SEGURO_SALUD</th>\n",
       "      <th>DIAS_AUSENTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9757</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9709</th>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SEXO  EDAD  NIVEL_EDUCATIVO  ETNIA  TUVO_TRABAJO  TIPO_TRABAJADOR  \\\n",
       "9757     2    24               10      7             2              NaN   \n",
       "9709     1    78               11      1             2              NaN   \n",
       "2588     1    21                8      7             1              3.0   \n",
       "6451     2    22               10      7             2              NaN   \n",
       "518      1    43                6      7             1              3.0   \n",
       "\n",
       "      BUSCA_OTRO_TRABAJO  NIVEL_OCUPACION  REGISTRO_SUNAT  TAMANO_EMPRESA  \\\n",
       "9757                 NaN                4             NaN             NaN   \n",
       "9709                 NaN                4             NaN             NaN   \n",
       "2588                 2.0                1             2.0             3.0   \n",
       "6451                 NaN                4             NaN             NaN   \n",
       "518                  2.0                1             1.0             2.0   \n",
       "\n",
       "      ...  FRECUENCIA_PAGO  INGRESO_PRINCIPAL  LIMIT_MOVIMIENTO  LIMIT_VISION  \\\n",
       "9757  ...              NaN                NaN                 2             2   \n",
       "9709  ...              NaN                NaN                 2             2   \n",
       "2588  ...              1.0             1800.0                 2             2   \n",
       "6451  ...              NaN                NaN                 2             2   \n",
       "518   ...              1.0             4500.0                 2             2   \n",
       "\n",
       "      LIMIT_COMUNICACION  LIMIT_AUDICION  LIMIT_APRENDIZAJE  LIMIT_RELACION  \\\n",
       "9757                   2               2                  2               2   \n",
       "9709                   2               2                  2               2   \n",
       "2588                   2               2                  2               2   \n",
       "6451                   2               2                  2               2   \n",
       "518                    2               2                  2               2   \n",
       "\n",
       "      SEGURO_SALUD  DIAS_AUSENTE  \n",
       "9757             5           2.0  \n",
       "9709             1           2.0  \n",
       "2588             5           2.0  \n",
       "6451             5           2.0  \n",
       "518              6           2.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extraer_muestra.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuración ---\n",
    "RANDOM_STATE = 42  # Para reproducibilidad\n",
    "SAMPLE_SIZE = 25   # Número de registros a extraer\n",
    "ORIGINAL_CSV = '../data/raw/employ25_3-4-5.csv'  # Ruta a tu archivo original\n",
    "OUTPUT_CSV = 'new_data_sample.csv'   # Archivo de salida\n",
    "\n",
    "# --- Mapeo de nombres (el mismo que usaste en data_loader) ---\n",
    "column_mapping = {\n",
    "    \"C207\": \"SEXO\",\n",
    "    \"C208\": \"EDAD\",\n",
    "    \"C366\": \"NIVEL_EDUCATIVO\",\n",
    "    \"C377\": \"ETNIA\",\n",
    "    \"C303\": \"TUVO_TRABAJO\",\n",
    "    \"C310\": \"TIPO_TRABAJADOR\",\n",
    "    \"C335\": \"BUSCA_OTRO_TRABAJO\",\n",
    "    \"OCUP300\": \"NIVEL_OCUPACION\",\n",
    "    \"C312\": \"REGISTRO_SUNAT\",\n",
    "    \"C317\": \"TAMANO_EMPRESA\",\n",
    "    \"C333\": \"QUIERE_MAS_HORAS\",\n",
    "    \"C313\": \"LIBROS_CONTABLES\",\n",
    "    \"C338\": \"FRECUENCIA_PAGO\",\n",
    "    \"ingtrabw\": \"INGRESO_PRINCIPAL\",\n",
    "    \"C375_1\": \"LIMIT_MOVIMIENTO\",\n",
    "    \"C375_2\": \"LIMIT_VISION\",\n",
    "    \"C375_3\": \"LIMIT_COMUNICACION\",\n",
    "    \"C375_4\": \"LIMIT_AUDICION\",\n",
    "    \"C375_5\": \"LIMIT_APRENDIZAJE\",\n",
    "    \"C375_6\": \"LIMIT_RELACION\",\n",
    "    \"SEGURO1\": \"SEGURO_SALUD\",\n",
    "    \"C205\": \"DIAS_AUSENTE\"\n",
    "}\n",
    "\n",
    "# --- 1. Cargar datos originales ---\n",
    "print(f\"📂 Cargando datos originales de {ORIGINAL_CSV}...\")\n",
    "df_original = pd.read_csv(ORIGINAL_CSV, na_values=[\" \"])\n",
    "\n",
    "# --- 2. Limpieza básica ---\n",
    "print(\"🧹 Realizando limpieza básica...\")\n",
    "# Eliminar columnas completamente vacías\n",
    "df_original.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# --- 3. Extraer muestra aleatoria ---\n",
    "print(f\"🎲 Extrayendo {SAMPLE_SIZE} registros aleatorios...\")\n",
    "sample = df_original.sample(SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "# --- 4. Seleccionar y renombrar columnas relevantes ---\n",
    "print(\"🔍 Seleccionando y renombrando columnas relevantes...\")\n",
    "\n",
    "# Seleccionar solo las columnas que están en el mapeo\n",
    "available_columns = [col for col in column_mapping.keys() if col in sample.columns]\n",
    "sample = sample[available_columns]\n",
    "\n",
    "# Renombrar columnas a nombres descriptivos\n",
    "sample = sample.rename(columns=column_mapping)\n",
    "\n",
    "# --- 5. Guardar muestra ---\n",
    "print(f\"💾 Guardando muestra en {OUTPUT_CSV}...\")\n",
    "sample.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# --- 6. Resultado final ---\n",
    "print(\"\\n✅ Muestra creada exitosamente con nombres descriptivos!\")\n",
    "print(f\"• Total registros: {len(sample)}\")\n",
    "print(f\"• Columnas: {sample.columns.tolist()}\")\n",
    "print(\"\\n📝 Vista previa:\")\n",
    "display(sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6401c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Cargando datos originales de ../data/raw/employ25_3-4-5.csv...\n",
      "🧹 Realizando limpieza básica...\n",
      "🎲 Extrayendo 25 registros aleatorios...\n",
      "🔍 Seleccionando columnas relevantes...\n",
      "💾 Guardando muestra en new_data_sample_SINP.csv...\n",
      "\n",
      "✅ Muestra creada exitosamente!\n",
      "• Total registros: 25\n",
      "• Columnas: ['C207', 'C208', 'C366', 'C377', 'C303', 'C310', 'C335', 'OCUP300', 'C312', 'C317', 'C333', 'C313', 'C338', 'ingtrabw', 'C375_1', 'C375_2', 'C375_3', 'C375_4', 'C375_5', 'C375_6', 'SEGURO1', 'C205']\n",
      "\n",
      "📝 Vista previa:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C207</th>\n",
       "      <th>C208</th>\n",
       "      <th>C366</th>\n",
       "      <th>C377</th>\n",
       "      <th>C303</th>\n",
       "      <th>C310</th>\n",
       "      <th>C335</th>\n",
       "      <th>OCUP300</th>\n",
       "      <th>C312</th>\n",
       "      <th>C317</th>\n",
       "      <th>...</th>\n",
       "      <th>C338</th>\n",
       "      <th>ingtrabw</th>\n",
       "      <th>C375_1</th>\n",
       "      <th>C375_2</th>\n",
       "      <th>C375_3</th>\n",
       "      <th>C375_4</th>\n",
       "      <th>C375_5</th>\n",
       "      <th>C375_6</th>\n",
       "      <th>SEGURO1</th>\n",
       "      <th>C205</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9757</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9709</th>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6451</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      C207  C208  C366  C377  C303  C310  C335  OCUP300  C312  C317  ...  \\\n",
       "9757     2    24    10     7     2   NaN   NaN        4   NaN   NaN  ...   \n",
       "9709     1    78    11     1     2   NaN   NaN        4   NaN   NaN  ...   \n",
       "2588     1    21     8     7     1   3.0   2.0        1   2.0   3.0  ...   \n",
       "6451     2    22    10     7     2   NaN   NaN        4   NaN   NaN  ...   \n",
       "518      1    43     6     7     1   3.0   2.0        1   1.0   2.0  ...   \n",
       "\n",
       "      C338  ingtrabw  C375_1  C375_2  C375_3  C375_4  C375_5  C375_6  SEGURO1  \\\n",
       "9757   NaN       NaN       2       2       2       2       2       2        5   \n",
       "9709   NaN       NaN       2       2       2       2       2       2        1   \n",
       "2588   1.0    1800.0       2       2       2       2       2       2        5   \n",
       "6451   NaN       NaN       2       2       2       2       2       2        5   \n",
       "518    1.0    4500.0       2       2       2       2       2       2        6   \n",
       "\n",
       "      C205  \n",
       "9757   2.0  \n",
       "9709   2.0  \n",
       "2588   2.0  \n",
       "6451   2.0  \n",
       "518    2.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extraer_muestra.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuración ---\n",
    "RANDOM_STATE = 42  # Para reproducibilidad\n",
    "SAMPLE_SIZE = 25   # Número de registros a extraer\n",
    "ORIGINAL_CSV = '../data/raw/employ25_3-4-5.csv'  # Ruta a tu archivo original\n",
    "OUTPUT_CSV = 'new_data_sample_SINP.csv'   # Archivo de salida\n",
    "\n",
    "# --- 1. Cargar datos originales ---\n",
    "print(f\"📂 Cargando datos originales de {ORIGINAL_CSV}...\")\n",
    "df_original = pd.read_csv(ORIGINAL_CSV, na_values=[\" \"])\n",
    "\n",
    "# --- 2. Limpieza básica ---\n",
    "print(\"🧹 Realizando limpieza básica...\")\n",
    "# Eliminar columnas completamente vacías\n",
    "df_original.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# --- 3. Extraer muestra aleatoria ---\n",
    "print(f\"🎲 Extrayendo {SAMPLE_SIZE} registros aleatorios...\")\n",
    "sample = df_original.sample(SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "# --- 4. Seleccionar columnas relevantes ---\n",
    "# Las mismas columnas que usaste en tu proyecto original\n",
    "relevant_columns = [\n",
    "    \"C207\", \"C208\", \"C366\", \"C377\", \"C303\", \"C310\", \n",
    "    \"C335\", \"OCUP300\", \"C312\", \"C317\", \"C333\", \n",
    "    \"C313\", \"C338\", \"ingtrabw\", \"C375_1\", \"C375_2\", \n",
    "    \"C375_3\", \"C375_4\", \"C375_5\", \"C375_6\", \"SEGURO1\", \"C205\"\n",
    "]\n",
    "\n",
    "print(\"🔍 Seleccionando columnas relevantes...\")\n",
    "sample = sample[relevant_columns]\n",
    "\n",
    "# --- 5. Guardar muestra ---\n",
    "print(f\"💾 Guardando muestra en {OUTPUT_CSV}...\")\n",
    "sample.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# --- 6. Resultado final ---\n",
    "print(\"\\n✅ Muestra creada exitosamente!\")\n",
    "print(f\"• Total registros: {len(sample)}\")\n",
    "print(f\"• Columnas: {sample.columns.tolist()}\")\n",
    "print(\"\\n📝 Vista previa:\")\n",
    "display(sample.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
